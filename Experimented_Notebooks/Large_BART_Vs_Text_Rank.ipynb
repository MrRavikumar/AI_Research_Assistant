{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1CfdMIgMiRIU1x_Sk3INR212p-9712GMO","authorship_tag":"ABX9TyP2OciLHkJ2FfL/OxlzJ+Bd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"gpuClass":"standard","widgets":{"application/vnd.jupyter.widget-state+json":{"13ba2fe5991845aabe4e2f8d440d3e23":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_72a08a76cee54b60847c0e398588ec65","IPY_MODEL_f90427277917429599290a04834c05c5","IPY_MODEL_746eb1d1df8d4628bc21c2493cf32530"],"layout":"IPY_MODEL_f87d971e82f64233bc10fc79cb8284c4"}},"72a08a76cee54b60847c0e398588ec65":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_59abfe9a025446a985d1a62e82e9b640","placeholder":"​","style":"IPY_MODEL_0bee57fa1f674629bb5543f642e3ad87","value":"100%"}},"f90427277917429599290a04834c05c5":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c66db0061c9748a883cb15732572edbd","max":3,"min":0,"orientation":"horizontal","style":"IPY_MODEL_709cefcfc91d4d4587982782a9cd200f","value":3}},"746eb1d1df8d4628bc21c2493cf32530":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5067a9a78f724861819b7fd880d422cb","placeholder":"​","style":"IPY_MODEL_0a17a496dd6c47879730d750a1f5a019","value":" 3/3 [00:02&lt;00:00,  1.50it/s]"}},"f87d971e82f64233bc10fc79cb8284c4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"59abfe9a025446a985d1a62e82e9b640":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0bee57fa1f674629bb5543f642e3ad87":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c66db0061c9748a883cb15732572edbd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"709cefcfc91d4d4587982782a9cd200f":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5067a9a78f724861819b7fd880d422cb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0a17a496dd6c47879730d750a1f5a019":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7c6e11de5a504f949f12f330c722fa92":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_95844013972b4b3e9301f36128b6c6c4","IPY_MODEL_ea3bf7a2059d434da1798aa468e90b31","IPY_MODEL_171c7458429a4721840ba5ec7405ceea"],"layout":"IPY_MODEL_a91b9427162448c4b4e738add695d65a"}},"95844013972b4b3e9301f36128b6c6c4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e922bf5522f41d1af2bd9efb2ea82fa","placeholder":"​","style":"IPY_MODEL_98682e5b6b46488490f0e71a0dde2745","value":"Downloading (…)lve/main/config.json: 100%"}},"ea3bf7a2059d434da1798aa468e90b31":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ef227e8dc5c46558914cab2dcf5d2a5","max":1585,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f88d36a4e1fc4d6bb30ac2696a865013","value":1585}},"171c7458429a4721840ba5ec7405ceea":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ebf4e45ff322477fa0d8bfa968d7c874","placeholder":"​","style":"IPY_MODEL_4a8a986edc7d4b24bb4a23b10e4111d2","value":" 1.58k/1.58k [00:00&lt;00:00, 44.0kB/s]"}},"a91b9427162448c4b4e738add695d65a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7e922bf5522f41d1af2bd9efb2ea82fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"98682e5b6b46488490f0e71a0dde2745":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3ef227e8dc5c46558914cab2dcf5d2a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f88d36a4e1fc4d6bb30ac2696a865013":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ebf4e45ff322477fa0d8bfa968d7c874":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4a8a986edc7d4b24bb4a23b10e4111d2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4db57b82476f4886841440b622bfb403":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_22f2fcb41a05442ba468a14801fade64","IPY_MODEL_b876b3561c3c44c09a4b73dacfa3e211","IPY_MODEL_717fc9f245d54db1923e7ecbbe5da524"],"layout":"IPY_MODEL_1ef356f43d554e89ac177ec08f3a5c02"}},"22f2fcb41a05442ba468a14801fade64":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_e00583194d584754bb44450b16d15eec","placeholder":"​","style":"IPY_MODEL_22a03e8aab2449f9b815c15a54071c38","value":"Downloading pytorch_model.bin: 100%"}},"b876b3561c3c44c09a4b73dacfa3e211":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_53801e1ac5b44a13b7887972f5d8315f","max":1625270765,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f42c1b13d16b41c8a899badc2cd59c4a","value":1625270765}},"717fc9f245d54db1923e7ecbbe5da524":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_182e5a07127347ceb0b8612e19a3b23b","placeholder":"​","style":"IPY_MODEL_ff2572622cb44be186055124c5cea362","value":" 1.63G/1.63G [01:14&lt;00:00, 21.9MB/s]"}},"1ef356f43d554e89ac177ec08f3a5c02":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e00583194d584754bb44450b16d15eec":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"22a03e8aab2449f9b815c15a54071c38":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"53801e1ac5b44a13b7887972f5d8315f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f42c1b13d16b41c8a899badc2cd59c4a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"182e5a07127347ceb0b8612e19a3b23b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ff2572622cb44be186055124c5cea362":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d5a53d50a6034e219344a34082c29cd3":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1f9de8f5910e4b63b900fc6a36908b63","IPY_MODEL_eddd691da2934cfb999ff74f8d3864ef","IPY_MODEL_4d002402082740098cdb7f75d9a9968a"],"layout":"IPY_MODEL_6e266e4f0d71487a913dde5e6d24ad37"}},"1f9de8f5910e4b63b900fc6a36908b63":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_18a35aa00e1c4345bdb188cf36689f64","placeholder":"​","style":"IPY_MODEL_95d3eb33cf474e93981a3081f9224ea6","value":"Downloading (…)neration_config.json: 100%"}},"eddd691da2934cfb999ff74f8d3864ef":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d97b445b32e349ccadbf87a388abfb17","max":363,"min":0,"orientation":"horizontal","style":"IPY_MODEL_512c83b1d9db46569237dbbacca772d5","value":363}},"4d002402082740098cdb7f75d9a9968a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_335551623d8e43cf8ac69a7c4e32a935","placeholder":"​","style":"IPY_MODEL_1e96c7e3fe4945278578dbad10fde8d9","value":" 363/363 [00:00&lt;00:00, 10.1kB/s]"}},"6e266e4f0d71487a913dde5e6d24ad37":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"18a35aa00e1c4345bdb188cf36689f64":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95d3eb33cf474e93981a3081f9224ea6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d97b445b32e349ccadbf87a388abfb17":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"512c83b1d9db46569237dbbacca772d5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"335551623d8e43cf8ac69a7c4e32a935":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e96c7e3fe4945278578dbad10fde8d9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9a68d134623430a95825d966178f2a8":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_6510687d7c004f49ae1ee7725e3df604","IPY_MODEL_6fd2ae5fa69d43889dfac78e38029722","IPY_MODEL_21efadfa4ca24245b3287a439aa02f16"],"layout":"IPY_MODEL_37f8c1ba5e6848cb8c7ef515f891359a"}},"6510687d7c004f49ae1ee7725e3df604":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_69de1613d9734684b067d8d0ad182277","placeholder":"​","style":"IPY_MODEL_e3eaafe3c7db46e6a429d2450d21e445","value":"Downloading (…)olve/main/vocab.json: 100%"}},"6fd2ae5fa69d43889dfac78e38029722":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_76f517074d4843e4a5636b5bb716d95d","max":898823,"min":0,"orientation":"horizontal","style":"IPY_MODEL_92b3343b1a4f4d6b9faf59d950aaa669","value":898823}},"21efadfa4ca24245b3287a439aa02f16":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_805cc6a78a7741f78c1f7d1fa5a38495","placeholder":"​","style":"IPY_MODEL_abc049a1150c4e61a593171e41f7152a","value":" 899k/899k [00:00&lt;00:00, 970kB/s]"}},"37f8c1ba5e6848cb8c7ef515f891359a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69de1613d9734684b067d8d0ad182277":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e3eaafe3c7db46e6a429d2450d21e445":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"76f517074d4843e4a5636b5bb716d95d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"92b3343b1a4f4d6b9faf59d950aaa669":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"805cc6a78a7741f78c1f7d1fa5a38495":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"abc049a1150c4e61a593171e41f7152a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3b06830e50b542ba92cab4b48b05a60e":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b65ae9341bc643fa83e57ab251504d87","IPY_MODEL_7947f3fc5cdc46c4aace80e093c207c3","IPY_MODEL_3a6fa6efd8a541ce8d05a2d2c6b5c4a7"],"layout":"IPY_MODEL_1c61bac175e44752a155c1411572042d"}},"b65ae9341bc643fa83e57ab251504d87":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ce79d13c46204543a2b62c967e856049","placeholder":"​","style":"IPY_MODEL_39cc87791fbf428bb581229d64dbb76d","value":"Downloading (…)olve/main/merges.txt: 100%"}},"7947f3fc5cdc46c4aace80e093c207c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b4ae163999644d7dacf75f3aef783d27","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b58dcafe043340e8b392be9d916ac3fb","value":456318}},"3a6fa6efd8a541ce8d05a2d2c6b5c4a7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5072eda9d8545c1bed909d89d8199e4","placeholder":"​","style":"IPY_MODEL_1e8dcf00983c4e11b57184c3e4fcecb3","value":" 456k/456k [00:00&lt;00:00, 619kB/s]"}},"1c61bac175e44752a155c1411572042d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ce79d13c46204543a2b62c967e856049":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"39cc87791fbf428bb581229d64dbb76d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b4ae163999644d7dacf75f3aef783d27":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b58dcafe043340e8b392be9d916ac3fb":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d5072eda9d8545c1bed909d89d8199e4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1e8dcf00983c4e11b57184c3e4fcecb3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","source":["!pip install transformers\n","!pip install datasets\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umQp4I3pOna4","executionInfo":{"status":"ok","timestamp":1679108733604,"user_tz":-330,"elapsed":33788,"user":{"displayName":"BE Project VAR","userId":"07773159486892367749"}},"outputId":"c32b9254-ef2b-42e2-8e56-4ad1d1a7e856"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.27.1-py3-none-any.whl (6.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.7/6.7 MB\u001b[0m \u001b[31m32.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.2-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m58.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.2 tokenizers-0.13.2 transformers-4.27.1\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting datasets\n","  Downloading datasets-2.10.1-py3-none-any.whl (469 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m469.0/469.0 KB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiohttp\n","  Downloading aiohttp-3.8.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting xxhash\n","  Downloading xxhash-3.2.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.2/212.2 KB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyarrow>=6.0.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (9.0.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (6.0)\n","Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (2.27.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.9/dist-packages (from datasets) (1.4.4)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from datasets) (1.22.4)\n","Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (4.65.0)\n","Requirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.9/dist-packages (from datasets) (2023.3.0)\n","Requirement already satisfied: huggingface-hub<1.0.0,>=0.2.0 in /usr/local/lib/python3.9/dist-packages (from datasets) (0.13.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.9/dist-packages (from datasets) (23.0)\n","Collecting multiprocess\n","  Downloading multiprocess-0.70.14-py39-none-any.whl (132 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m132.9/132.9 KB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting responses<0.19\n","  Downloading responses-0.18.0-py3-none-any.whl (38 kB)\n","Collecting dill<0.3.7,>=0.3.0\n","  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 KB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting frozenlist>=1.1.1\n","  Downloading frozenlist-1.3.3-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (158 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.8/158.8 KB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (2.0.12)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.9/dist-packages (from aiohttp->datasets) (22.2.0)\n","Collecting async-timeout<5.0,>=4.0.0a3\n","  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n","Collecting multidict<7.0,>=4.5\n","  Downloading multidict-6.0.4-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (114 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m114.2/114.2 KB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting aiosignal>=1.1.2\n","  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n","Collecting yarl<2.0,>=1.0\n","  Downloading yarl-1.8.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (264 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m264.6/264.6 KB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (3.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0.0,>=0.2.0->datasets) (4.5.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (2022.12.7)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (3.4)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.19.0->datasets) (1.26.15)\n","Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas->datasets) (2022.7.1)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.9/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.15.0)\n","Installing collected packages: xxhash, multidict, frozenlist, dill, async-timeout, yarl, responses, multiprocess, aiosignal, aiohttp, datasets\n","Successfully installed aiohttp-3.8.4 aiosignal-1.3.1 async-timeout-4.0.2 datasets-2.10.1 dill-0.3.6 frozenlist-1.3.3 multidict-6.0.4 multiprocess-0.70.14 responses-0.18.0 xxhash-3.2.0 yarl-1.8.2\n"]}]},{"cell_type":"markdown","source":["##**Pre-Train Large BART CNN**"],"metadata":{"id":"KwQFIUImVAxB"}},{"cell_type":"code","source":["import torch\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","\n","# Load BART model and tokenizer\n","model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","\n","# Define a function to generate a summary\n","def generate_summary(input_text, model, tokenizer):\n","    # Tokenize the input text\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt')\n","\n","    # Generate the summary using the BART model\n","    summary_ids = model.generate(input_ids, num_beams=4, max_length=100, early_stopping=True)\n","    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","    return summary\n","\n","# Example usage\n","input_text = 'Over the past few decades, interest in theories and algorithms for face recognition has been growing rapidly. Video surveillance, criminal identification, building access control, and unmanned and autonomous vehicles are just a few examples of concrete applications that are gaining attraction among industries. Various techniques are being developed including local, holistic, and hybrid approaches, which provide a face image description using only a few face image features or the whole facial features. The main contribution of this survey is to review some well-known techniques for each approach and to give the taxonomy of their categories. In the paper, a detailed comparison between these techniques is exposed by listing the advantages and the disadvantages of their schemes in terms of robustness, accuracy, complexity, and discrimination. One interesting feature mentioned in the paper is about the database used for face recognition. An overview of the most commonly used databases, including those of supervised and unsupervised learning, is given. Numerical results of the most interesting techniques are given along with the context of experiments and challenges handled by these techniques. Finally, a solid discussion is given in the paper about future directions in terms of techniques to be used for face recognition.'\n","summary = generate_summary(input_text, model, tokenizer)\n","print('Generated Summary:', summary)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":214,"referenced_widgets":["7c6e11de5a504f949f12f330c722fa92","95844013972b4b3e9301f36128b6c6c4","ea3bf7a2059d434da1798aa468e90b31","171c7458429a4721840ba5ec7405ceea","a91b9427162448c4b4e738add695d65a","7e922bf5522f41d1af2bd9efb2ea82fa","98682e5b6b46488490f0e71a0dde2745","3ef227e8dc5c46558914cab2dcf5d2a5","f88d36a4e1fc4d6bb30ac2696a865013","ebf4e45ff322477fa0d8bfa968d7c874","4a8a986edc7d4b24bb4a23b10e4111d2","4db57b82476f4886841440b622bfb403","22f2fcb41a05442ba468a14801fade64","b876b3561c3c44c09a4b73dacfa3e211","717fc9f245d54db1923e7ecbbe5da524","1ef356f43d554e89ac177ec08f3a5c02","e00583194d584754bb44450b16d15eec","22a03e8aab2449f9b815c15a54071c38","53801e1ac5b44a13b7887972f5d8315f","f42c1b13d16b41c8a899badc2cd59c4a","182e5a07127347ceb0b8612e19a3b23b","ff2572622cb44be186055124c5cea362","d5a53d50a6034e219344a34082c29cd3","1f9de8f5910e4b63b900fc6a36908b63","eddd691da2934cfb999ff74f8d3864ef","4d002402082740098cdb7f75d9a9968a","6e266e4f0d71487a913dde5e6d24ad37","18a35aa00e1c4345bdb188cf36689f64","95d3eb33cf474e93981a3081f9224ea6","d97b445b32e349ccadbf87a388abfb17","512c83b1d9db46569237dbbacca772d5","335551623d8e43cf8ac69a7c4e32a935","1e96c7e3fe4945278578dbad10fde8d9","c9a68d134623430a95825d966178f2a8","6510687d7c004f49ae1ee7725e3df604","6fd2ae5fa69d43889dfac78e38029722","21efadfa4ca24245b3287a439aa02f16","37f8c1ba5e6848cb8c7ef515f891359a","69de1613d9734684b067d8d0ad182277","e3eaafe3c7db46e6a429d2450d21e445","76f517074d4843e4a5636b5bb716d95d","92b3343b1a4f4d6b9faf59d950aaa669","805cc6a78a7741f78c1f7d1fa5a38495","abc049a1150c4e61a593171e41f7152a","3b06830e50b542ba92cab4b48b05a60e","b65ae9341bc643fa83e57ab251504d87","7947f3fc5cdc46c4aace80e093c207c3","3a6fa6efd8a541ce8d05a2d2c6b5c4a7","1c61bac175e44752a155c1411572042d","ce79d13c46204543a2b62c967e856049","39cc87791fbf428bb581229d64dbb76d","b4ae163999644d7dacf75f3aef783d27","b58dcafe043340e8b392be9d916ac3fb","d5072eda9d8545c1bed909d89d8199e4","1e8dcf00983c4e11b57184c3e4fcecb3"]},"id":"Y2CkRBraVADW","executionInfo":{"status":"ok","timestamp":1679108851421,"user_tz":-330,"elapsed":117824,"user":{"displayName":"BE Project VAR","userId":"07773159486892367749"}},"outputId":"28f24e37-2701-4bf0-a245-ec3981cebb9e"},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/1.58k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7c6e11de5a504f949f12f330c722fa92"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading pytorch_model.bin:   0%|          | 0.00/1.63G [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4db57b82476f4886841440b622bfb403"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)neration_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d5a53d50a6034e219344a34082c29cd3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9a68d134623430a95825d966178f2a8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3b06830e50b542ba92cab4b48b05a60e"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Generated Summary: Interest in theories and algorithms for face recognition has been growing rapidly. Video surveillance, criminal identification, building access control, and unmanned and autonomous vehicles are just a few examples of concrete applications. Various techniques are being developed including local, holistic, and hybrid approaches, which provide a face image description using only a few face image features or the whole facial features.\n"]}]},{"cell_type":"markdown","source":["## **Summary:**\n","Interest in theories and algorithms for face recognition has been growing rapidly. Video surveillance, criminal identification, building access control, and unmanned and autonomous vehicles are just a few examples of concrete applications. Various techniques are being developed including local, holistic, and hybrid approaches, which provide a face image description using only a few face image features or the whole facial features."],"metadata":{"id":"u7Q-mGu6sBYi"}},{"cell_type":"markdown","source":["### **Fine Tune Large Bart**"],"metadata":{"id":"uQVizDXgbwbl"}},{"cell_type":"code","source":["import torch\n","from transformers import BartTokenizer, BartForConditionalGeneration\n","from torch.utils.data import DataLoader\n","from datasets import load_dataset\n","\n","# Load the arxiv dataset from the HuggingFace Datasets library\n","dataset = load_dataset('scientific_papers', 'arxiv')\n","\n","# Load the BART model and tokenizer\n","model = BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n","tokenizer = BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n","\n","# Define a function to generate a summary for a given input text using the BART model\n","def generate_summary(input_text, max_length=100):\n","    input_ids = tokenizer.encode(input_text, return_tensors='pt', truncation=True, max_length=512)\n","    summary_ids = model.generate(input_ids, max_length=max_length, early_stopping=True)\n","    return tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n","\n","# Define a function to fine-tune the BART model on a dataset of input/target summary pairs\n","def fine_tune(model, tokenizer, train_dataset, val_dataset, batch_size=4, epochs=2, lr=1e-5):\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n","    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n","    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n","    criterion = torch.nn.CrossEntropyLoss()\n","    for epoch in range(epochs):\n","        model.train()\n","        train_loss = 0\n","        for batch in train_loader:\n","            input_ids = tokenizer.batch_encode_plus(batch['input_text'], padding=True, truncation=True, max_length=512, return_tensors='pt')['input_ids']\n","            target_ids = tokenizer.batch_encode_plus(batch['target_summary'], padding=True, truncation=True, max_length=128, return_tensors='pt')['input_ids']\n","            optimizer.zero_grad()\n","            # Generate the summary using the BART model\n","            outputs = model(input_ids, labels=target_ids)\n","            loss = outputs.loss\n","            loss.backward()\n","            optimizer.step()\n","            train_loss += loss.item()\n","        model.eval()\n","        val_loss = 0\n","        with torch.no_grad():\n","            for batch in val_loader:\n","                input_ids = tokenizer.batch_encode_plus(batch['input_text'], padding=True, truncation=True, max_length=512, return_tensors='pt')['input_ids']\n","                target_ids = tokenizer.batch_encode_plus(batch['target_summary'], padding=True, truncation=True, max_length=128, return_tensors='pt')['input_ids']\n","                # Generate the summary using the BART model\n","                outputs = model(input_ids, labels=target_ids)\n","                loss = outputs.loss\n","                val_loss += loss.item()\n","        print(f'Epoch {epoch+1}: Train Loss={train_loss:.4f} Val Loss={val_loss:.4f}')\n","    return model\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":86,"referenced_widgets":["13ba2fe5991845aabe4e2f8d440d3e23","72a08a76cee54b60847c0e398588ec65","f90427277917429599290a04834c05c5","746eb1d1df8d4628bc21c2493cf32530","f87d971e82f64233bc10fc79cb8284c4","59abfe9a025446a985d1a62e82e9b640","0bee57fa1f674629bb5543f642e3ad87","c66db0061c9748a883cb15732572edbd","709cefcfc91d4d4587982782a9cd200f","5067a9a78f724861819b7fd880d422cb","0a17a496dd6c47879730d750a1f5a019"]},"id":"CCcjmOjrVAO4","executionInfo":{"status":"ok","timestamp":1679080627945,"user_tz":-330,"elapsed":35428,"user":{"displayName":"BE Project VAR","userId":"07773159486892367749"}},"outputId":"fbaf2b5f-dd41-4177-ab22-4f36b70f095e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["WARNING:datasets.builder:Found cached dataset scientific_papers (/root/.cache/huggingface/datasets/scientific_papers/arxiv/1.1.1/306757013fb6f37089b6a75469e6638a553bd9f009484938d8f75a4c5e84206f)\n"]},{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/3 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"13ba2fe5991845aabe4e2f8d440d3e23"}},"metadata":{}}]},{"cell_type":"code","source":["# Split the dataset into a training and validation set\n","train_dataset = dataset['train'][:10]\n","val_dataset = dataset['train'][10:12]"],"metadata":{"id":"uv3rqBUTa8tp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import pandas as pd\n","tr_data = pd.DataFrame.from_dict(train_dataset)\n","vd_data = pd.DataFrame.from_dict(val_dataset)\n","train_dataset = []\n","val_dataset = []\n","for i in range(len(tr_data)):\n","  train_dataset.append({'input_text': tr_data['article'].iloc[i], 'target_summary': tr_data['abstract'].iloc[i]})\n","for i in range(len(vd_data)):\n","  val_dataset.append({'input_text': vd_data['article'].iloc[i], 'target_summary': vd_data['abstract'].iloc[i]})"],"metadata":{"id":"k2jFVsnkffnn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Fine-tune the BART model on the training dataset\n","model = fine_tune(model, tokenizer, train_dataset, val_dataset, batch_size=2, epochs=2, lr=1e-5)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rMyFWeWgpDji","executionInfo":{"status":"ok","timestamp":1679081075216,"user_tz":-330,"elapsed":432825,"user":{"displayName":"BE Project VAR","userId":"07773159486892367749"}},"outputId":"7676c94e-fcbf-4e92-f40b-8adfed9a72f6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1: Train Loss=23.0249 Val Loss=4.3987\n","Epoch 2: Train Loss=17.1408 Val Loss=4.3239\n"]}]},{"cell_type":"code","source":["input_text = \"Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk. In this paper, we focus on facial biometric systems, which are widely used in surveillance and access control. We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. Our investigation focuses on white-box face-recognition systems, but we also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.\"\n","summary = generate_summary(input_text)\n","print('Summary: ', summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BrWtldKYtS0a","executionInfo":{"status":"ok","timestamp":1679081156677,"user_tz":-330,"elapsed":45264,"user":{"displayName":"BE Project VAR","userId":"07773159486892367749"}},"outputId":"c8706de2-c53c-470d-d149-ae2c8d1606cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Summary:  We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. We also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.\n"]}]},{"cell_type":"markdown","source":["##**Actual:**\n","Machine learning is enabling a myriad innovations, including new algorithms for cancer diagnosis and self-driving cars. The broad use of machine learning makes it important to understand the extent to which machine-learning algorithms are subject to attack, particularly when used in applications where physical security or safety is at risk. In this paper, we focus on facial biometric systems, which are widely used in surveillance and access control. We define and investigate a novel class of attacks: attacks that are physically realizable and inconspicuous, and allow an attacker to evade recognition or impersonate another individual. We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. Our investigation focuses on white-box face-recognition systems, but we also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection."],"metadata":{"id":"CpIbTCTlCtVR"}},{"cell_type":"markdown","source":["##**Summary:** \n","We develop a systematic method to automatically generate such attacks, which are realized through printing a pair of eyeglass frames. When worn by the attacker whose image is supplied to a state-of-the-art face-recognition algorithm, the eyeglasses allow her to evade being recognized or to impersonate another individual. We also demonstrate how similar techniques can be used in black-box scenarios, as well as to avoid face detection.\n"],"metadata":{"id":"aNkPCdNBCV8W"}},{"cell_type":"code","source":["input_text = \"Abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern abstractive summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document, summary) pairs. To account for data scarcity, we used a modern pretrained abstractive summarizer BART (Lewis et al., 2020), which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel algorithm based on GPT-2 (Radford et al., 2019) language model perplexity scores, that operates within the low resource regime. On feeding the compressed documents to BART, we observe a 6.0 ROUGE-L improvement. Our method also beats several competitive salience detection baselines. Furthermore, the identified salient sentences tend to agree with an independent human labeling by domain experts.\"\n","summary = generate_summary(input_text, max_length=200)\n","print('Summary: ', summary)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7AfT6GCcw9ag","executionInfo":{"status":"ok","timestamp":1679081971146,"user_tz":-330,"elapsed":28921,"user":{"displayName":"BE Project VAR","userId":"07773159486892367749"}},"outputId":"66c812ee-37df-4539-af09-02dc7d1a16bc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Summary:   abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary.\n"]}]},{"cell_type":"markdown","source":["##**Actual:**\n","Abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern abstractive summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. In this paper, we study a challenging low-resource setting of summarizing long legal briefs with an average source document length of 4268 words and only 120 available (document, summary) pairs. To account for data scarcity, we used a modern pretrained abstractive summarizer BART (Lewis et al., 2020), which only achieves 17.9 ROUGE-L as it struggles with long documents. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel algorithm based on GPT-2 (Radford et al., 2019) language model perplexity scores, that operates within the low resource regime. On feeding the compressed documents to BART, we observe a 6.0 ROUGE-L improvement. Our method also beats several competitive salience detection baselines. Furthermore, the identified salient sentences tend to agree with an independent human labeling by domain experts."],"metadata":{"id":"c8MRroycEvZx"}},{"cell_type":"markdown","source":["## **Summary:**\n","abstractive summarization is the task of compressing a long document into a coherent short document while retaining salient information. Modern summarization methods are based on deep neural networks which often require large training datasets. Since collecting summarization datasets is an expensive and time-consuming task, practical industrial settings are usually low-resource. We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary."],"metadata":{"id":"NDZd8A9NFt7X"}},{"cell_type":"code","source":["pip install PyPDF2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N2h6xfWdHpr1","executionInfo":{"status":"ok","timestamp":1679082581732,"user_tz":-330,"elapsed":10101,"user":{"displayName":"BE Project VAR","userId":"07773159486892367749"}},"outputId":"86706cfe-f19b-48c1-ccef-2ae5c809d9a0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting PyPDF2\n","  Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m232.6/232.6 KB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing_extensions>=3.10.0.0 in /usr/local/lib/python3.9/dist-packages (from PyPDF2) (4.5.0)\n","Installing collected packages: PyPDF2\n","Successfully installed PyPDF2-3.0.1\n"]}]},{"cell_type":"code","source":["pip install GingerIt"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4PJaYvcLKXfN","executionInfo":{"status":"ok","timestamp":1679087911273,"user_tz":-330,"elapsed":8771,"user":{"displayName":"BE Project VAR","userId":"07773159486892367749"}},"outputId":"58ca918b-8c7e-4d47-b674-25dca6cac3c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: GingerIt in /usr/local/lib/python3.9/dist-packages (0.9.0)\n","Requirement already satisfied: cloudscraper<2.0.0,>=1.2.66 in /usr/local/lib/python3.9/dist-packages (from GingerIt) (1.2.69)\n","Requirement already satisfied: requests-toolbelt>=0.9.1 in /usr/local/lib/python3.9/dist-packages (from cloudscraper<2.0.0,>=1.2.66->GingerIt) (0.10.1)\n","Requirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.9/dist-packages (from cloudscraper<2.0.0,>=1.2.66->GingerIt) (2.25.1)\n","Requirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.9/dist-packages (from cloudscraper<2.0.0,>=1.2.66->GingerIt) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.2->cloudscraper<2.0.0,>=1.2.66->GingerIt) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.2->cloudscraper<2.0.0,>=1.2.66->GingerIt) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.2->cloudscraper<2.0.0,>=1.2.66->GingerIt) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.9.2->cloudscraper<2.0.0,>=1.2.66->GingerIt) (1.26.15)\n"]}]},{"cell_type":"markdown","source":["Text Rank Algorithm "],"metadata":{"id":"ZiEDMXjfcEsj"}},{"cell_type":"code","source":["import PyPDF2\n","import gensim\n","import spacy\n","from gensim.summarization import summarize\n","from gensim.utils import simple_preprocess\n","\n","# Define a custom stopwords list\n","custom_stopwords = [\"research\", \"paper\", \"study\", \"results\", \"conclusions\"]\n","\n","# Load a spaCy model for named entity recognition\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","# Open the PDF file in read-binary mode\n","with open('/content/drive/MyDrive/Research_Papers/VideoSumSpringer.pdf', 'rb') as f:\n","    # Create a PDF reader object\n","    pdf_reader = PyPDF2.PdfReader(f)\n","\n","    # Get the number of pages in the PDF file\n","    num_pages = len(pdf_reader.pages)\n","\n","    # Loop through each page in the PDF file\n","    paper_text = \"\"\n","    for page_num in range(num_pages):\n","        # Get the page object for the current page\n","        page = pdf_reader.pages[page_num]\n","\n","        # Extract the text from the page object\n","        text = page.extract_text()\n","\n","        # Add the text to the paper_text variable\n","        paper_text += text\n","\n","# Preprocess the text\n","sentences = gensim.summarization.textcleaner.split_sentences(paper_text)\n","tokens = [simple_preprocess(sentence) for sentence in sentences]\n","\n","# Remove custom stopwords from the tokens\n","tokens_without_stopwords = [[token for token in sentence_tokens if token not in custom_stopwords] for sentence_tokens in tokens]\n","\n","# Convert the tokens back to sentences\n","sentences_without_stopwords = [\" \".join(sentence_tokens) for sentence_tokens in tokens_without_stopwords]\n","\n","# Use spaCy for named entity recognition\n","ner_sentences = []\n","for sentence in sentences_without_stopwords:\n","    doc = nlp(sentence)\n","    ner_sentence = \"\"\n","    for token in doc:\n","        if token.ent_type_:\n","            ner_sentence += token.ent_type_ + \" \"\n","        else:\n","            ner_sentence += token.text + \" \"\n","    ner_sentences.append(ner_sentence)\n","\n","# Join the sentences into a single string\n","paper_text_without_stopwords = \". \".join(ner_sentences)\n","\n","# Perform text summarization using gensim's TextRank algorithm\n","sentences = summarize(paper_text_without_stopwords, ratio=0.2, split=True)\n","\n","# Post-processing: Correct spelling and grammar errors and add missing punctuation\n","# Here, we use the GingerIt library for automatic spelling and grammar correction\n","from gingerit.gingerit import GingerIt\n","corrector = GingerIt()\n","\n","# Process each sentence in the summary\n","summary = \"\"\n","for sentence in sentences:\n","    if sentence is not None:\n","        # Correct spelling and grammar errors and add missing punctuation\n","        corrections = corrector.parse(sentence)\n","        if 'Corrections' in corrections:\n","            sentence = corrections['Corrections']\n","        else:\n","            sentence = corrections['result']\n","        sentence = sentence.strip()\n","        if not sentence.endswith(\".\"):\n","            sentence += \".\"\n","        summary += sentence + \" \"\n","\n","# Print the summary\n","print(summary)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5JGvqI6pEssB","executionInfo":{"status":"ok","timestamp":1679084178368,"user_tz":-330,"elapsed":63489,"user":{"displayName":"BE Project VAR","userId":"07773159486892367749"}},"outputId":"9459b5d5-5d47-481c-f648-773983690fa0"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Video transcript extraction and summarization. Single frame videos require the viewer to watch the entire thing to fully. Algorithms such as text rank and seq seq models. PERSON PERSON algorithms and models the system proposed in this pa. Per converts audio chunks from input videos into transcripts. Marization model based on natural language processing and transfer. The developed model accepts user supplied video links as input. And outputs summary like description of video. Key words NLP transfer learning large ORG ORG pre trained. One can use our system to summarize the transcripts or captions of the video. Propose video transcript summarizes that. Takes YouTube links as input, extracts the transcript if not provided and gives. The summarized transcript using hugging, face transformers. This model only works for LANGUAGE. Input video transcripts. Proposes bertsum model for summarizing. The proposed model for summarization is trained. On combination of ORG DATE mail wikihow and how datasets. On CARDINAL CARDINAL dataset and evaluated using the rouge score and content. Datasets among training and testing and observed the best CARDINAL. From podcast extract text summarization to summarize the transcript and. Return the audio linked with the text summary. They produced summaries for the audio transcripts of several podcasts to gain. Algorithm to handle podcast summaries explicitly using these text summaries video transcripts. NLP methods to extract and summarize information from audio and video data. Extractive text summarizing is a method of summarizing that draws sum. Transcribed text they attach video strings based on subtitles using the algebraic. It uses fewer computing resources and ORG ORG any prior training data. Pre trained language models for deep neural networks are growing in pop. There are much different transfer learning models available for NLP tasks. An encoder decoder model that transforms all NLP issues into text. ORG model pre trained in the LANGUAGE language and has been refined. Town to train it ORG builds a model to retrieve the original text. Vision and natural language processing frequently build on pre trained models. Tasks hampered by lack of data and inadequate model generalization in the. We could use the model that had already been trained for CARDINAL job too. Strategies will be covered along with how they can be used for video transcript. Text summarization model. Convert video to audio we will be using the moviepy python library which. Video into the corresponding wav audio format. Text out of the video file or more accurately creating a textual version of the. The transcription process begins with an audio. Serves as the raw text converted to the video transcript using the writer's. Text summarization we use automatic obstructive summarization to create. Text summaries from the transcripts, we applied the transformers package for. Using the dataset of summarized YouTube transcripts. Which we described in section we fine tuned Facebook ORG large model to. How dataset, there are videos in this YouTube collection totaling. . Videos make up the entire dataset that was used and the split is shown below. The dataset processing process is followed before the model selection as well. As the model training process the preprocessing steps include. Text data file to video I do. Tabled evaluation of different pre trained models on how dataset. For choosing the best model for our proposed system we tested different trans. For learning pretrained models like small base ORG large ORG PRODUCT. ORG exam using how dataset and predicted the summary using this mod. Ability to calculate the rouge and PERSON score evaluation metrics as discussed. Large ORG gave an overall better score than all the models considered. Selected the ORG large ORG model for fine tuning process. The ORG model has. Already been trained in LANGUAGE language and has been fine tuned using ORG. Tune this ORG large ORG model and train it on our training dataset described. Description of ORG large ORG model. ORG model is explained in detail in section. Summary pairings the ORG large ORG model has been fine tuned. We fined tuned the ORG large ORG model from the section on the dataset from. Fast a package created to make deep learning more approachable video transcript. Algorithm fine tuning and model training. Output trained model. Build the obstructive summarization model of fine tuning. Training the model. Training the model. Book ORG large ORG model and prepare data for training. Fine tuning the model and preprocessing data for training now we will. Your raw data into modelable information. Training the model and evaluate in this phase, we prepare our ORG. Model for training by wrapping it in blurry object and using. The result of the prediction or summary that is given by our trained model. . The fine tuned large ORG can model is evaluated on the testing dataset. Reference summaries are provided by the rouge package which ORG ORDINAL Varun Mehta et al. Embeddings from PERSON that have already been trained Bert score evaluating. Text generation compares words in candidate and reference sentences based on. The rouge scores for all the models before fine tuning are compiled in. Fine tuned large ORG ORG model. PERSON score obtained for each epoch, while training which is shown in fig. We provided YouTube video url and converted. The video speech  transcripts and applied summarization using our model to. Generated and the video url is displayed in tabular form in a table. Table fine tuned ORG large ORG evaluation. PERSON score video transcript. The table predicted summary of the video. Video url extracted transcript predicted summary. Therefore, this proposes a system to extract video, audio and transcripts. Comparative analysis of some transformer models. Was done and the ORG large ORG model gave superior out of all the. The ORG large ORG model and the testing dataset mentioned in the section. Url we used this model to summarize the video. We can expand the data on which our model is trained. Video transcript Summarizer. Video CARDINAL and summarization using. Video summarization. Abstractive text summarization using sequences. \n"]}]},{"cell_type":"markdown","source":["## **Summary:**\n","Video transcript extraction and summarization. Single frame videos require the viewer to watch the entire thing to fully. Algorithms such as text rank and seq seq models. PERSON PERSON algorithms and models the system proposed in this pa. Per converts audio chunks from input videos into transcripts. Marization model based on natural language processing and transfer. The developed model accepts user supplied video links as input. And outputs summary like description of video. Key words NLP transfer learning large ORG ORG pre trained. One can use our system to summarize the transcripts or captions of the video. Propose video transcript summarizes that. Takes YouTube links as input, extracts the transcript if not provided and gives. The summarized transcript using hugging, face transformers. This model only works for LANGUAGE. Input video transcripts. Proposes bertsum model for summarizing. The proposed model for summarization is trained. On combination of ORG DATE mail wikihow and how datasets. On CARDINAL CARDINAL dataset and evaluated using the rouge score and content. Datasets among training and testing and observed the best CARDINAL. From podcast extract text summarization to summarize the transcript and. Return the audio linked with the text summary. They produced summaries for the audio transcripts of several podcasts to gain. Algorithm to handle podcast summaries explicitly using these text summaries video transcripts. NLP methods to extract and summarize information from audio and video data. Extractive text summarizing is a method of summarizing that draws sum. Transcribed text they attach video strings based on subtitles using the algebraic. It uses fewer computing resources and ORG ORG any prior training data. Pre trained language models for deep neural networks are growing in pop. There are much different transfer learning models available for NLP tasks. An encoder decoder model that transforms all NLP issues into text. ORG model pre trained in the LANGUAGE language and has been refined. Town to train it ORG builds a model to retrieve the original text. Vision and natural language processing frequently build on pre trained models. Tasks hampered by lack of data and inadequate model generalization in the. We could use the model that had already been trained for CARDINAL job too. Strategies will be covered along with how they can be used for video transcript. Text summarization model. Convert video to audio we will be using the moviepy python library which. Video into the corresponding wav audio format. Text out of the video file or more accurately creating a textual version of the. The transcription process begins with an audio. Serves as the raw text converted to the video transcript using the writer's. Text summarization we use automatic obstructive summarization to create. Text summaries from the transcripts, we applied the transformers package for. Using the dataset of summarized YouTube transcripts. Which we described in section we fine tuned Facebook ORG large model to. How dataset, there are videos in this YouTube collection totaling. . Videos make up the entire dataset that was used and the split is shown below. The dataset processing process is followed before the model selection as well. As the model training process the preprocessing steps include. Text data file to video I do. Tabled evaluation of different pre trained models on how dataset. For choosing the best model for our proposed system we tested different trans. For learning pretrained models like small base ORG large ORG PRODUCT. ORG exam using how dataset and predicted the summary using this mod. Ability to calculate the rouge and PERSON score evaluation metrics as discussed. Large ORG gave an overall better score than all the models considered. Selected the ORG large ORG model for fine tuning process. The ORG model has. Already been trained in LANGUAGE language and has been fine tuned using ORG. Tune this ORG large ORG model and train it on our training dataset described. Description of ORG large ORG model. ORG model is explained in detail in section. Summary pairings the ORG large ORG model has been fine tuned. We fined tuned the ORG large ORG model from the section on the dataset from. Fast a package created to make deep learning more approachable video transcript. Algorithm fine tuning and model training. Output trained model. Build the obstructive summarization model of fine tuning. Training the model. Training the model. Book ORG large ORG model and prepare data for training. Fine tuning the model and preprocessing data for training now we will. Your raw data into modelable information. Training the model and evaluate in this phase, we prepare our ORG. Model for training by wrapping it in blurry object and using. The result of the prediction or summary that is given by our trained model. . The fine tuned large ORG can model is evaluated on the testing dataset. Reference summaries are provided by the rouge package which ORG ORDINAL Varun Mehta et al. Embeddings from PERSON that have already been trained Bert score evaluating. Text generation compares words in candidate and reference sentences based on. The rouge scores for all the models before fine tuning are compiled in. Fine tuned large ORG ORG model. PERSON score obtained for each epoch, while training which is shown in fig. We provided YouTube video url and converted. The video speech  transcripts and applied summarization using our model to. Generated and the video url is displayed in tabular form in a table. Table fine tuned ORG large ORG evaluation. PERSON score video transcript. The table predicted summary of the video. Video url extracted transcript predicted summary. Therefore, this proposes a system to extract video, audio and transcripts. Comparative analysis of some transformer models. Was done and the ORG large ORG model gave superior out of all the. The ORG large ORG model and the testing dataset mentioned in the section. Url we used this model to summarize the video. We can expand the data on which our model is trained. Video transcript Summarizer. Video CARDINAL and summarization using. Video summarization. Abstractive text summarization using sequences. "],"metadata":{"id":"D8rwvTDBN62k"}},{"cell_type":"code","source":["import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","from collections import Counter\n","from gensim.summarization import summarize\n","\n","# Load the spacy model\n","nlp = spacy.load(\"en_core_web_sm\")\n","\n","text = \"The MAJ and DMD rules are compared in Figure 4. For each N, four trials of the discrete HMM training were performed to find the mean and standard deviation of DMD OCP. From the 71h order polynomial fitting, the best OCP is 89.7% when N=l4, and the worst OCP is 86.6% when N=6. The MAJ OCP is always 81.7%. For CMD rule, we search for the optimum setting starting from N=G=d=l. Figure 5 shows the CMD OCP on d. The peak OCP is 99.0% when d=8. Away from N=G=l and d=8, the CMD OCP decays monotonically with G and N as shown in Figure 6. Four trials of training were performed to find the means and standard deviations for each setting. Thus the best OCPs of the MAJ, DMD, and CMD rules can be compared to the single-frame face recognition, as summarized\"\n","\n","# Tokenize the text and remove stop words\n","doc = nlp(text)\n","tokens = [token.text for token in doc if not token.is_stop and token.is_alpha]\n","\n","# Get the most frequent words and their counts\n","word_freq = Counter(tokens)\n","top_words = word_freq.most_common(10)\n","\n","# Print the most frequent words and their counts\n","print(\"Top 10 most frequent words:\")\n","for word, count in top_words:\n","    print(f\"{word}: {count}\")\n","\n","# Summarize the text using gensim's TextRank algorithm\n","summary = summarize(text)\n","\n","# Print the summary\n","print(\"Summary:\")\n","print(summary)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IeDKYvBJGrZ8","executionInfo":{"status":"ok","timestamp":1679085061503,"user_tz":-330,"elapsed":2619,"user":{"displayName":"BE Project VAR","userId":"07773159486892367749"}},"outputId":"9cfd4a07-faac-4a3c-c5a8-79ffe6dc755f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Top 10 most frequent words:\n","OCP: 7\n","N: 5\n","CMD: 4\n","MAJ: 3\n","DMD: 3\n","Figure: 3\n","G: 3\n","rules: 2\n","compared: 2\n","trials: 2\n","Summary:\n","For each N, four trials of the discrete HMM training were performed to find the mean and standard deviation of DMD OCP.\n","Thus the best OCPs of the MAJ, DMD, and CMD rules can be compared to the single-frame face recognition, as summarized\n"]}]},{"cell_type":"markdown","source":["## **Summary:**\n","We thus attempt to compress these long documents by identifying salient sentences in the source which best ground the summary, using a novel algorithm based on GPT-2 (Radford et al., 2019) language model perplexity scores, that operates within the low resource regime."],"metadata":{"id":"mFy6MnZMP5R4"}},{"cell_type":"code","source":["import re\n","import nltk\n","import heapq\n","import gensim\n","import numpy as np\n","\n","nltk.download('punkt')\n","\n","def preprocess_text(text):\n","    # remove citations\n","    text = re.sub(r'\\[\\d+\\]', '', text)\n","    # remove newlines\n","    text = re.sub(r'\\n', ' ', text)\n","    # split text into sentences\n","    sentences = nltk.sent_tokenize(text)\n","    # remove punctuations and convert to lowercase\n","    preprocessed_sentences = []\n","    for sentence in sentences:\n","        sentence = re.sub('[^a-zA-Z0-9]', ' ', sentence).lower()\n","        preprocessed_sentences.append(sentence)\n","    return preprocessed_sentences\n","\n","def summarize_text(text, num_sentences=3):\n","    # preprocess text\n","    preprocessed_sentences = preprocess_text(text)\n","    # create document matrix\n","    documents = [gensim.utils.simple_preprocess(sentence) for sentence in preprocessed_sentences]\n","    dictionary = gensim.corpora.Dictionary(documents)\n","    bow_corpus = [dictionary.doc2bow(doc) for doc in documents]\n","    tfidf = gensim.models.TfidfModel(bow_corpus)\n","    corpus_tfidf = tfidf[bow_corpus]\n","    # create LSI model and apply it to the corpus\n","    lsi_model = gensim.models.LsiModel(corpus_tfidf, id2word=dictionary, num_topics=len(documents))\n","    corpus_lsi = lsi_model[corpus_tfidf]\n","    # get sentence scores\n","    sentence_scores = []\n","    for i, sentence in enumerate(corpus_lsi):\n","        score = 0\n","        for _, val in sentence:\n","            score += val\n","        sentence_scores.append((i, score))\n","    # sort sentence scores and get top n sentences\n","    top_sentences = heapq.nlargest(num_sentences, sentence_scores, key=lambda x: x[1])\n","    top_sentences.sort(key=lambda x: x[0])\n","    # generate summary\n","    summary = ' '.join([preprocessed_sentences[i] for i, _ in top_sentences])\n","    # extract numeric facts from summary\n","    numeric_facts = re.findall(r'\\d+\\.?\\d*', summary)\n","    # remove duplicates and convert to float\n","    numeric_facts = list(set([float(num) for num in numeric_facts]))\n","    # create summary with numeric facts\n","    summary_with_facts = summary + '\\n\\nNumeric Facts: ' + ', '.join([str(num) for num in numeric_facts])\n","    return summary_with_facts\n","\n","# Example usage\n","# text = \"\"\"\n","# The results show that the new algorithm outperforms existing algorithms on several benchmark datasets. For example, on the MNIST dataset, our algorithm achieved a classification accuracy of 98.5%, compared to the state-of-the-art accuracy of 97.8%. On the CIFAR-10 dataset, our algorithm achieved an accuracy of 92.3%, which is 1.5% higher than the state-of-the-art accuracy of 90.8%. Furthermore, our algorithm has a lower computational complexity, which makes it more suitable for real-time applications.\n","# \"\"\"\n","\n","# input text\n","text = \"The MAJ and DMD rules are compared in Figure 4. For each N, four trials of the discrete HMM training were performed to find the mean and standard deviation of DMD OCP. From the 71h order polynomial fitting, the best OCP is 89.7% when N=l4, and the worst OCP is 86.6% when N=6. The MAJ OCP is always 81.7%. For CMD rule, we search for the optimum setting starting from N=G=d=l. Figure 5 shows the CMD OCP on d. The peak OCP is 99.0% when d=8. Away from N=G=l and d=8, the CMD OCP decays monotonically with G and N as shown in Figure 6. Four trials of training were performed to find the means and standard deviations for each setting. Thus the best OCPs of the MAJ, DMD, and CMD rules can be compared to the single-frame face recognition, as summarized\"\n","\n","\n","summary = summarize_text(text, num_sentences=6)\n","print(summary)\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-KsKzdzlTROq","executionInfo":{"status":"ok","timestamp":1679089485258,"user_tz":-330,"elapsed":399,"user":{"displayName":"BE Project VAR","userId":"07773159486892367749"}},"outputId":"bb55e4cf-7c0e-410d-f75e-ff8a7d6efd92"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["for each n  four trials of the discrete hmm training were performed to find the mean and standard deviation of dmd ocp  the maj ocp is always 81 7   for cmd rule  we search for the optimum setting starting from n g d l  away from n g l and d 8  the cmd ocp decays monotonically with g and n as shown in figure 6  four trials of training were performed to find the means and standard deviations for each setting  thus the best ocps of the maj  dmd  and cmd rules can be compared to the single frame face recognition  as summarized\n","\n","Numeric Facts: 8.0, 81.0, 6.0, 7.0\n"]},{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}]},{"cell_type":"markdown","source":["## **Actual:**\n","The MAJ and DMD rules are compared in Figure 4. For each N, four trials of the discrete HMM training were performed to find the mean and standard deviation of DMD OCP. From the 71h order polynomial fitting, the best OCP is 89.7% when N=l4, and the worst OCP is 86.6% when N=6. The MAJ OCP is always 81.7%. For CMD rule, we search for the optimum setting starting from N=G=d=l. Figure 5 shows the CMD OCP on d. The peak OCP is 99.0% when d=8. Away from N=G=l and d=8, the CMD OCP decays monotonically with G and N as shown in Figure 6. Four trials of training were performed to find the means and standard deviations for each setting. Thus the best OCPs of the MAJ, DMD, and CMD rules can be compared to the single-frame face recognition, as summarized"],"metadata":{"id":"hY9vcnJZh4qU"}},{"cell_type":"markdown","source":["## **Summary:**\n","The maj and dmd rules are compared in figure 4  for each n  four trials of the discrete hmm training were performed to find the mean and standard deviation of dmd ocp  from the 71h order polynomial fitting  the best ocp is 89 7  when n l4  and the worst ocp is 86 6  when n 6  the maj ocp is always 81 7   for cmd rule  we search for the optimum setting starting from n g d l  figure 5 shows the cmd ocp on d  the peak ocp is 99 0  when d 8  away from n g l and d 8  the cmd ocp decays monotonically with g and n as shown in figure 6  four trials of training were performed to find the means and standard deviations for each setting  thus the best ocps of the maj  dmd  and cmd rules can be compared to the single frame face recognition  as summarized"],"metadata":{"id":"b31XUBNTbP8K"}},{"cell_type":"code","source":[],"metadata":{"id":"MlnNNvYTiewN"},"execution_count":null,"outputs":[]}]}